Start with a search on machine learning in Google Scholar here Page on google.com. Look at likely sounding papers and get a feel for the related keywords. The refine your search in Google Scholar using the keywords. Once you have identified some papers you want, search for them in downloadable PDF format by entering the title followed by PDF in regular Google search. Where that fails, try the university webpage of the authors who sometimes provide links to their papers there. If both fail, buy the papers you want from the publisher of the journal they appear in.

For the latest, try arXiv. Though not peer reviewed, you’ll find that people put their papers there early and often, even the machine-learning “giants”.

Specifically, check these out - they get updated everyday except weekends:

Computer Vision and Pattern Recognition
http://arxiv.org/list/cs.CV/recent
Learning authors/titles recent submissions
http://arxiv.org/list/cs.LG/recent
Machine Learning
http://arxiv.org/list/stat.ml/recent
Note there is some overlap.

Some people prefer to look here, to avoid being overrun by the barrage of new papers coming out :

Arxiv Sanity Preserver
http://www.arxiv-sanity.com/

Try ArXiv (has a free repository of these papers), Google Scholar, and specialized journals (Pattern Recognition, Computational Statistics and Data Analysis, Journal of Machine Learning Research...).

The first question here is what is your end goal? You can read my answer here to get some pointers.

I think what makes reading papers harder is the presentation - sometimes the papers have too much technical content, and the language is often terse. As a result, you need to put in a lot of effort to get the first few paragraphs, and by that time you start to feel exhausted and lose interest.

Now, probably the most important technique to deal with this is to do multiple passes. Often, papers will have a lot of peripheral content and technical details other than the main ideas.

Skimming through the paper should give you the main ideas and the bigger theme. Specifically, try to see what is the problem that they are solving and what is the general framework they are using (graphical models, neural networks, etc.) The problem will typically be stated in an intuitive language in the introduction and more formally after “Related Works” section. Don’t go through the “Related Works” section now, unless you know that the paper is very similar to one of the papers discussed in the section, and you have read that paper. A high-level idea of the approach may be written down explicitly, or may be obtained from figures or equations, or by skimming through the rest of the paper quickly by searching for keywords.
Then, you can do a more detailed pass, where you can now try to see how different sections relate to the theme. Skip the proofs and derivations here. Just understand what all concepts are being introduced / used and how. Also look at the “Experiments” section at this point to get better sense of the ideas.
Finally, you should go over the technical details (e.g. the mathematics) if you are required to read the paper to that depth.
Other important "tricks" that have worked for me include:

Group study: Reading papers with someone makes it a lot more easier. What may be hard to understand by reading alone, may be easier when you discuss and explain ideas to each other.
Making notes: Sometimes, papers have too much information which is hard to keep up with. Making notes as you go along helps you remember things better. Plus, you tend to write only the important stuff in a concise manner while making notes, which makes more sense to you.


If there is a github implementation can I just read (Abstract, conclusion, then jump to code)?
Example:
1) Title, abstract then conclusion

2) Go for the pictures and equation description

3) If you still like the paper try to work out on the equations, Lemmas

4) If you still there, try to replicate the experiment

5) Change it a bit and do another paper :)

If there is a github implementation just do steps 1, 2 and jump to the code
