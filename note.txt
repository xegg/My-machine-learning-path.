Mon Feb 13 19:55:19 CST 2017:
有时候真的不知道做些什么好, 就写下今天一天学习到了什么吧,
完成了coursera的神经网络章节, 尝试了一下kaggle, 正确明天
进化成初级

Tue Feb 14 22:33:31 CST 2017:
今天只学习了矩阵的分解, 因为想要养成早前的习惯, 明天去买一个保温杯,
这样早上起来的时候就能够喝到热水了, 就能够起床干一些有意义的事情了.


Wed Feb 15 20:26:54 CST 2017:
也不知道自己应该每天做什么来耍kaggle, 每天的事情都比较多, 但是自己总是能够腾出
时间来做一点机器学习相关的东西吧, 下一个kernel来跑一下, 看下大家是怎么做处理的, 
我应该用来做一些什么事情, 等等这些都挺好的.

今天晚上就把那个入门的课程完成吧

feature selection
Here a few ideas for selecting parameters:

Grid search - build models using all possible combinations of hyperparameters, and see which performs best. In sklearn, GridSearchCV provides a handy way to do this.
Randomized search - if your model accepts many parameters, then it may be impractical to try all possible combinations of model hyperparameters, because it would take forever. Trying a few random sets of parameters can make your model 'good enough', or at least let you know roughly what parameter values result in better performance.
Find historic instances of models fit to a similar dataset, and see what parameter values were used there. In the very least these values can be useful initial guesses for a grid or randomized search.
Datasets with many features can take a while to fit a model to, so reducing the number of features might be a good idea if you want to perform a comprehensive parameter optimization. parameter optimization by reducing the number of features.

Marco mentioned one family of these - subset selection - where only the parameters that are useful in making predictions are included in the model. Other methods to lower dimensionality are principal component analysis and linear discriminant analysis.
Fri Feb 17 23:10:16 CST 2017:
今天学习了线性代数的xxxinput out模型和线性代数在图形学中的应用, 可能因为嗓子哑了, 昨天晚上并没有学习什么知识, 并且现在的精神还不错,
争取明天调整过来吧. 早一点睡觉


Sat Feb 18 19:44:37 CST 2017:
今天感冒了, 看了点ted视频, 学习了Rn的子空间, 就没有做其他事情了, 感觉好累, 好没有自信啊.

Sun Feb 19 22:12:28 CST 2017:
今天去姐姐那里看了一伙TED视频, 收集了两个很精彩的视频, 一个是被拒100天的项目, 很有意思, 
很能改变一个人, 还有一个就是excel之父, 最后几句话很有意思, 利用技术搭建原型, 解决自己的需求,
希望自己有一天也能创造这样杀手级的应用,

今天还看了一会行列式, 进步很慢, 但是希望自己每天都能够有进步.

Mon Feb 20 21:57:36 CST 2017:
今天看完了行列式, 因为起来的比较晚, 又被琐碎的事情耽误了点时间, 没有做什么事情


Tue Feb 21 23:08:24 CST 2017:
今天什么事情都没有做, 看了一个向量空间的序...

Wed Feb 22 22:59:38 CST 2017:
希望自己的技术能够变的更好, 比斌哥, echo, 华远他们都好.

Thu Feb 23 19:49:46 CST 2017:
看了一下需求工程相关的内容, 终于看完了向量空间的第一章了.

Fri Feb 24 23:08:20 CST 2017:
今天下载了R工具包, 打算大概的了解一下R, 在做选择是因该使用python还是R, 希望明天后天能够在
公交成商胃肠hand-on R programming这本书

Sat Feb 25 23:47:39 CST 2017:
今天在大巴上看了5章的R语言教程,其实坐在中间还是能够看很多书的, 争取明天也能够看5章的书籍, 
大致的完成R语言的教程.

Mon Feb 27 20:11:14 CST 2017:
暂时放弃了R语言的学习, 没有学习到很多新的知识, 只知道pie chart其实很少人使用, 那其他很多东西
都没有python好, 因为自己比较注重实用, 还是使用python吧, 有很多python的库可以使用, 看了一会
think stats, 发现统计学的知识真的很多, 逛wiki啊百科就可以学习到很多的知识.

Tue Feb 28 21:13:34 CST 2017:
今天完成了kaggle的泰坦尼克的生存预测, 还挺有意思的,分为几大模块, 探索数据, 数据处理,
使用模型, 最后进行预测, 今天看下大家写的评论

Wed Mar  1 19:22:18 CST 2017:
今天继续看了会大家写的教程, 熟悉了工具的用法, 看到一个比较有趣的就是去wiki百科查询哪些名单的人
生存了下来...

Thu Mar  2 23:22:54 CST 2017:
今天对pokemon的数据做了一点分析工作, 还挺有意思的.
Fri Mar  3 22:55:48 CST 2017:
今天继续开始看赤裸裸的统计学...
Sat Mar  4 22:33:17 CST 2017:
继续看赤裸裸的经济学, 非常的过瘾


Sun Mar  5 23:40:55 CST 2017:
今天看了深入浅出统计学, 知道如何通过概率表查看缺失值, 学会了
几个简单的分布, z-score等概念.

Mon Mar  6 20:57:34 CST 2017:
看了两章数据如何欺骗我们, 争取看完深入浅出机器学习, 了解一些hadoop那一套东西

Tue Mar  7 23:07:39 CST 2017:
Nothing 其实什么都没有做也应该记录下来, 这样的话就能判断自己是否应该辞职认真的学习.

Wed Mar  8 22:13:37 CST 2017:
今天探索了一下kaggle的一个新的数据集, 胡乱的探索了一下数据集, 变量间的关系.
还是有时间能够学习一点机器学习的知识的,只是太少了, 每天2个小时, 中午30分钟, 
真的希望辞职, 全职做这个东西.

Fri Mar 9 11:15:34 CST 2017:
今天看了一伙书, 感觉自己的进度真的好慢呀.

Fri Mar 10 22:42:35 CST 2017:
几天下了很多电影, 打算休息一天, 不看书.

Sat Mar 11 19:24:21 CST 2017:
0h 看了欢乐好声音, 罗曼蒂克消亡史, 言叶之庭, 时空恋旅人, 海边的曼切斯特, 血战钢铁岭...

Sun Mar 12 23:03:59 CST 2017:
2h 好好的一个周末, 一天看了几步电影, 一天睡了15个小时的觉.

Mon Mar 13 23:49:48 CST 2017:
每天看3个小时的书, 今天看了data scientist as work其中两个人的访谈.

Tue Mar 14 23:51:07 CST 2017:
看完了赤裸裸的经济学, 内容还是太浅了, 知识还是要在实践中掌握.

Wed Mar 15 23:24:39 CST 2017:
开始看信号与噪音, 还挺有意思的书.

Fri Mar 17 23:26:53 CST 2017:
这两天折腾了很多mac下的效率工具, 如果以后自己都用mac做开发环境的话, 还是挺值得下载一下的,看书的时间就
变的少了很多.

Sun Mar 18 21:01:12 CST 2017:
今天看了第四公名, 真的应该开始注重隐私的问题了.

Sun Mar 19 22:41:31 CST 2017:
0h, 这两天看了8部电影, 没有学习任何东西...
